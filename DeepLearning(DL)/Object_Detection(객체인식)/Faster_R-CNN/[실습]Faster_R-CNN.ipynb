{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "56f36e7a",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/carki/Desktop/Dev/codeit/codeit/lib/python3.13/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Downloading from https://www.kaggle.com/api/v1/datasets/download/s076923/pytorch-transformer?dataset_version_number=4...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 916M/916M [01:33<00:00, 10.3MB/s] "
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Extracting files...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Path to dataset files: /Users/carki/.cache/kagglehub/datasets/s076923/pytorch-transformer/versions/4\n"
     ]
    }
   ],
   "source": [
    "import kagglehub\n",
    "import os\n",
    "\n",
    "\n",
    "# Download latest version\n",
    "path = kagglehub.dataset_download(\"s076923/pytorch-transformer\")\n",
    "print(\"Path to dataset files:\", path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "08526141",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 먼저, 압축 파일(pytorch-transformer.zip)을 지정된 경로(현재 디렉토리)로 압축 해제합니다.\n",
    "import shutil\n",
    "\n",
    "# shutil.move(path, '/Users/carki/Desktop/Dev/codeit/common/data')\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "9025f097",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 작업 디렉토리를 변경합니다.\n",
    "import os\n",
    "os.chdir(\"/Users/carki/Desktop/Dev/codeit/common/data/4/datasets\")  # 작업할 데이터셋 폴더로 이동 (실행 환경에 따라 경로가 달라질 수 있음)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b4e8c21",
   "metadata": {},
   "source": [
    "```json\n",
    "{\n",
    "\t\"info\": {\n",
    "\t\t\"year\": 2021,\n",
    "\t\t\"version\": \"1.0\",\n",
    "\t\t\"description\": \"For object detection\",\n",
    "\t\t\"date_created\": \"2021\"\n",
    "\t},\n",
    "\t\"images\": [\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000001.jpg\",\n",
    "\t\t\t\"id\": 1,\n",
    "\t\t\t\"height\": 480,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000002.jpg\",\n",
    "\t\t\t\"id\": 2,\n",
    "\t\t\t\"height\": 426,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000003.jpg\",\n",
    "\t\t\t\"id\": 3,\n",
    "\t\t\t\"height\": 428,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000004.jpg\",\n",
    "\t\t\t\"id\": 4,\n",
    "\t\t\t\"height\": 425,\n",
    "\t\t\t\"width\": 640\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"date_captured\": \"2021\",\n",
    "\t\t\t\"file_name\": \"000000000005.jpg\",\n",
    "\t\t\t\"id\": 5,\n",
    "\t\t\t\"height\": 640,\n",
    "\t\t\t\"width\": 481\n",
    "\t\t}\n",
    "\t],\n",
    "\t\"licenses\": [\n",
    "\t\t{\n",
    "\t\t\t\"id\": 1,\n",
    "\t\t\t\"name\": \"GNU General Public License v3.0\",\n",
    "\t\t\t\"url\": \"https://github.com/zhiqwang/yolov5-rt-stack/blob/master/LICENSE\"\n",
    "\t\t}\n",
    "\t],\n",
    "\t\"type\": \"instances\",\n",
    "\t\"annotations\": [\n",
    "\t\t{\n",
    "\t\t\t\"segmentation\": [\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t1.0799999999999272,\n",
    "\t\t\t\t\t187.69008000000002,\n",
    "\t\t\t\t\t612.66976,\n",
    "\t\t\t\t\t187.69008000000002,\n",
    "\t\t\t\t\t612.66976,\n",
    "\t\t\t\t\t473.53008000000005,\n",
    "\t\t\t\t\t1.0799999999999272,\n",
    "\t\t\t\t\t473.53008000000005\n",
    "\t\t\t\t]\n",
    "\t\t\t],\n",
    "\t\t\t\"area\": 174816.81699840003,\n",
    "\t\t\t\"iscrowd\": 0,\n",
    "\t\t\t\"image_id\": 1,\n",
    "\t\t\t\"bbox\": [\n",
    "\t\t\t\t1.0799999999999272,\n",
    "\t\t\t\t187.69008000000002,\n",
    "\t\t\t\t611.5897600000001,\n",
    "\t\t\t\t285.84000000000003\n",
    "\t\t\t],\n",
    "\t\t\t\"category_id\": 19,\n",
    "\t\t\t\"id\": 1\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"segmentation\": [\n",
    "\t\t\t\t[\n",
    "\t\t\t\t\t311.73024,\n",
    "\t\t\t\t\t4.310159999999996,\n",
    "\t\t\t\t\t631.0102400000001,\n",
    "\t\t\t\t\t4.310159999999996,\n",
    "\t\t\t\t\t631.0102400000001,\n",
    "\t\t\t\t\t232.99032,\n",
    "\t\t\t\t\t311.73024,\n",
    "\t\t\t\t\t232.99032\n",
    "\t\t\t\t]\n",
    "\t\t\t],\n",
    "\t\t\t\"area\": 73013.00148480001,\n",
    "\t\t\t\"iscrowd\": 0,\n",
    "\t\t\t\"image_id\": 1,\n",
    "\t\t\t\"bbox\": [\n",
    "\t\t\t\t311.73024,\n",
    "\t\t\t\t4.310159999999996,\n",
    "\t\t\t\t319.28000000000003,\n",
    "\t\t\t\t228.68016\n",
    "\t\t\t],\n",
    "\t\t\t\"category_id\": 50,\n",
    "\t\t\t\"id\": 2\n",
    "\t\t},\n",
    "        ],\n",
    "\t\"categories\": [\n",
    "\t\t{\n",
    "\t\t\t\"id\": 1,\n",
    "\t\t\t\"name\": \"0\",\n",
    "\t\t\t\"supercategory\": \"0\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"id\": 2,\n",
    "\t\t\t\"name\": \"1\",\n",
    "\t\t\t\"supercategory\": \"1\"\n",
    "\t\t},\n",
    "\t\t{\n",
    "\t\t\t\"id\": 3,\n",
    "\t\t\t\"name\": \"2\",\n",
    "\t\t\t\"supercategory\": \"2\"\n",
    "\t\t},\n",
    "    ]\n",
    "}\n",
    "```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "7c418ba9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os              # 파일 및 디렉토리 경로를 다루기 위한 표준 라이브러리\n",
    "import json            # JSON 파일을 읽고 쓰기 위한 표준 라이브러리\n",
    "import torch           # 딥러닝 라이브러리 PyTorch (텐서 연산 등)\n",
    "from PIL import Image  # 이미지를 다루기 위한 Pillow 라이브러리\n",
    "from torch.utils.data import Dataset  # PyTorch의 Dataset 클래스를 상속받기 위한 모듈\n",
    "\n",
    "# COCO 데이터셋의 JSON 파일을 직접 파싱하기 위한 사용자 정의 클래스\n",
    "class CustomCOCO:\n",
    "    def __init__(self, annotation_file):\n",
    "\n",
    "        # JSON 파일을 읽어서 데이터를 불러온다.\n",
    "        with open(annotation_file, 'r') as f:\n",
    "            self.data = json.load(f)\n",
    "\n",
    "        # 이미지 정보 ---> 이미지 'id'를 key로 하는 dict로 저장\n",
    "        self.images = {img[\"id\"]: img for img in self.data.get(\"images\", [])}\n",
    "\n",
    "        # Annotation 정보를 이미지 'id'로 그룹화\n",
    "        self.annotation = {}\n",
    "\n",
    "        for ann in self.data.get('annotations', []):\n",
    "            img_id = ann['image_id']\n",
    "\n",
    "            if img_id not in self.annotation:\n",
    "                self.annotation[img_id] = []\n",
    "            \n",
    "            self.annotation[img_id].append(ann)\n",
    "\n",
    "        # 카테고리 정보를 cat 'id'를 키로 하는 dict로 저장\n",
    "        self.cats = { cat['id']: cat for cat in self.data.get('categories', []) }\n",
    "        # self.cats = {}\n",
    "        # for cat in self.data.get('categories', []):\n",
    "        #     self.cats[cat['id']] = cat\n",
    "\n",
    "# 이미지 id들을 통해서 이미지 정보들을 불러온다\n",
    "def loadImgs(self, ids):\n",
    "    return [self.images[i] for i in ids if i in self.images]\n",
    "\n",
    "# 이미지 id를 통해서 Annotation id를 불러온다\n",
    "def getAnnIds(self, imgIds):\n",
    "    ann_ids = []\n",
    "    for img_id in imgIds:\n",
    "        if img_id in self.annotation:\n",
    "            ann_ids.extend([ann['id'] for ann in self.annotation[img_id]])\n",
    "\n",
    "    return ann_ids\n",
    "\n",
    "# Annotation id를 이용해서 실제 annotation 정보를 불러온다.\n",
    "def loadAnns(self, annIds):\n",
    "    anns = []\n",
    "    for ann in self.data.get('annotations', []):\n",
    "        if ann['id'] in annIds:\n",
    "            anns.append(ann)\n",
    "\n",
    "    return anns"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e32a0f73",
   "metadata": {},
   "source": [
    "# Pytorch Dataset 클래스를 상속받아서 coco 데이터셋 클래스 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "id": "2ac7c097",
   "metadata": {},
   "outputs": [],
   "source": [
    "class cocoDataset(Dataset):\n",
    "    def __init__(self, root, train, transform=None):\n",
    "        super().__init__()\n",
    "        # root : 데이터셋 위치(최상위)\n",
    "        # train : 학습 데이터(True), 검증데이터(False)\n",
    "\n",
    "        # dir = 'train' if train else 'val'\n",
    "        if train:\n",
    "            dir = 'train'\n",
    "        else:\n",
    "            dir = 'val'\n",
    "\n",
    "        # anno file의 위치를 지정\n",
    "        annotation_file = os.path.join(root, 'annotations', f'{dir}_annotations.json')\n",
    "\n",
    "        # CustomCOCO 클래스 지정\n",
    "        self.coco = CustomCOCO(annotation_file=annotation_file)\n",
    "\n",
    "        # image file 위치\n",
    "        self.image_path = os.path.join(root, dir)\n",
    "\n",
    "        self.transform = transform\n",
    "\n",
    "        # coco 데이터셋의 카테고리 정보를 저장\n",
    "        # 0번은 배경정보로 지정\n",
    "        self.categories = { 0: 'background' }\n",
    "        \n",
    "        for cat_id, cat in self.coco.cats.items():\n",
    "            self.categories[cat_id] = cat['name']\n",
    "\n",
    "        # 이미지와 어노테이션 정보를 로드 ---> data 리스트에 저장\n",
    "        self.data = self._load_data()\n",
    "\n",
    "    def _load_data(self):\n",
    "        data = []\n",
    "\n",
    "        for _id, img_info in self.coco.images.items():\n",
    "            # 이미지 파일 가져오기\n",
    "            file_name = img_info['file_name']\n",
    "\n",
    "            # 이미지 파일의 전체 경로 가져오기\n",
    "            image_path = os.path.join(self.image_path, file_name)\n",
    "\n",
    "            # PIL를 사용해서 이미지를 로드 ---> RGB\n",
    "            image = Image.open(image_path).convert('RGB')\n",
    "\n",
    "            boxes = []\n",
    "            labels = []\n",
    "            anns = self.coco.annotation.get(_id, [])\n",
    "\n",
    "            for ann in anns:\n",
    "                x, y, w, h = ann['bbox']\n",
    "                boxes.append([x, y, x+w, y+h])\n",
    "                labels.append(ann['category_id'])\n",
    "\n",
    "            # target 값들을 dict 형태로 만들어서 전달\n",
    "            taget = {\n",
    "                'image_id': torch.LongTensor([_id]),\n",
    "                'boxes': torch.FloatTensor(boxes),\n",
    "                'labels': torch.LongTensor(labels)\n",
    "            }\n",
    "\n",
    "            data.append((image, taget))\n",
    "        \n",
    "        return data\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.data)\n",
    "        \n",
    "    def __getitem__(self, index):\n",
    "        image, target = self.data[index]\n",
    "        \n",
    "        if self.transform:\n",
    "            images = self.transform(image)\n",
    "        \n",
    "        return image, target"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f501f39e",
   "metadata": {},
   "source": [
    "# Dataloader 생성"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "id": "c5fdf62c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# torchvision 라이브러리에서 이미지 전처리 도구와 DataLoader를 임포트합니다.\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader\n",
    "\n",
    "# 배치 데이터를 생성할 때, 각 배치마다 데이터를 튜플 형태로 묶어주는 함수\n",
    "# coco 데이터 셋은 이미지 내에 여러 객체 정보가 담길 수 있으므로, 데이터의 길이가 다를 수 있음.\n",
    "def collator(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "transform = transforms.Compose(\n",
    "    [\n",
    "        transforms.PILToTensor(),\n",
    "        transforms.ConvertImageDtype(dtype=torch.float)\n",
    "    ]\n",
    ")\n",
    "\n",
    "train_dataset = cocoDataset(root='/Users/carki/Desktop/Dev/codeit/common/data/4/datasets/coco', train=True, transform=transform)\n",
    "test_dataset = cocoDataset(root='/Users/carki/Desktop/Dev/codeit/common/data/4/datasets/coco', train=False, transform=transform)\n",
    "\n",
    "train_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=4,\n",
    "                              shuffle=True,\n",
    "                              drop_last=True,\n",
    "                              collate_fn=collator)\n",
    "test_dataloader = DataLoader(dataset=train_dataset,\n",
    "                              batch_size=1,\n",
    "                              shuffle=False,\n",
    "                              drop_last=True,\n",
    "                              collate_fn=collator)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "89aba921",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(<PIL.Image.Image image mode=RGB size=640x471>,\n",
       " {'image_id': tensor([374990]),\n",
       "  'boxes': tensor([[242.1200, 182.9700, 427.7500, 399.7300]]),\n",
       "  'labels': tensor([2])})"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataset[101]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bc1df1ed",
   "metadata": {},
   "source": [
    "# Custom Collator가 필요한 이유"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67b12ea7",
   "metadata": {},
   "source": [
    "아래에 기본 collate 함수의 출력과 custom collate를 적용한 최종 출력 형태를 요약했습니다.\n",
    "\n",
    "- **기본 collate 함수 (원래 형태):**\n",
    "  - **출력:**  \n",
    "    - **이미지 텐서:** 모든 이미지가 동일한 크기라면, 자동으로 스택되어 하나의 텐서로 만들어집니다.  \n",
    "      예: `(batch_size, channels, height, width)`\n",
    "    - **타겟(라벨) 텐서/딕셔너리:** 동일한 방식으로 스택 또는 텐서 형태로 구성됩니다.\n",
    "  - **문제점:**  \n",
    "    - 이미지나 타겟의 크기가 서로 다르면 스택하는 과정에서 에러가 발생합니다.\n",
    "\n",
    "- **Custom collate 적용 후 (최종 원하는 형태):**\n",
    "  - **출력:**  \n",
    "    - **이미지 리스트:** 각 이미지가 개별적으로 리스트에 담깁니다.  \n",
    "      예: `(image1, image2, ...)`\n",
    "    - **타겟 리스트:** 각 이미지에 해당하는 타겟이 개별적으로 리스트에 담깁니다.  \n",
    "      예: `(target1, target2, ...)`\n",
    "  - **형태 요약:**  \n",
    "    - 최종 배치의 출력은 **((이미지1, 이미지2, ...), (target1, target2, ...))** 형태로 구성됩니다.\n",
    "  - **장점:**  \n",
    "    - 이미지나 타겟이 서로 다른 크기여도 그대로 유지할 수 있어, 모델 입력 전에 적절한 전처리(예: 패딩)를 적용하거나, 개별적으로 처리할 수 있습니다.\n",
    "\n",
    "이렇게 custom collate를 사용하면 서로 다른 크기의 데이터를 안전하게 배치로 묶어 모델에 입력할 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1329504",
   "metadata": {},
   "source": [
    "# 2개의 샘플 이미지 (각각 다른 크기의 tensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "0acde510",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "에러발생:  stack expects each tensor to be equal size, but got [3, 300, 300] at entry 0 and [3, 400, 400] at entry 1\n",
      "batch 이미지 수:  2\n",
      "1st 이미지 크기:  torch.Size([3, 300, 300])\n",
      "2st 이미지 크기:  torch.Size([3, 400, 400])\n",
      "1st target:  {'boxes': tensor([[ 50.,  50., 200., 200.]]), 'labels': tensor([1])}\n",
      "2st target:  {'boxes': tensor([[ 10.,  50., 200., 200.],\n",
      "        [ 10.,  60., 200., 200.]]), 'labels': tensor([1])}\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torch.utils.data import dataloader\n",
    "\n",
    "image1 = torch.randn(3, 300, 300)\n",
    "target1 = {\n",
    "    'boxes': torch.tensor([\n",
    "        [50, 50, 200, 200]], \n",
    "        dtype=torch.float32),\n",
    "    'labels': torch.tensor([1])\n",
    "}\n",
    "\n",
    "image2 = torch.randn(3, 400, 400)\n",
    "target2 = {\n",
    "    'boxes': torch.tensor(\n",
    "        [\n",
    "            [10, 50, 200, 200], \n",
    "            [10, 60, 200, 200]\n",
    "        ], \n",
    "        dtype=torch.float32\n",
    "    ),\n",
    "    'labels': torch.tensor([1])\n",
    "}\n",
    "\n",
    "sample_data = [(image1, target1), (image2, target2)]\n",
    "loader_without_collator = DataLoader(dataset=sample_data,\n",
    "                                     batch_size=2)\n",
    "\n",
    "# 에러발생:  stack expects each tensor to be equal size, but got [3, 300, 300] at entry 0 and [3, 400, 400] at entry 1\n",
    "try:\n",
    "    for batch in loader_without_collator:\n",
    "        images, target = batch\n",
    "        print('Batch img shape: ', images.shape)\n",
    "except Exception as e:\n",
    "    print('에러발생: ', e)\n",
    "\n",
    "def collator(batch):\n",
    "    return tuple(zip(*batch))\n",
    "\n",
    "loader_with_collator = DataLoader(dataset=sample_data,\n",
    "                                  batch_size=2, \n",
    "                                  collate_fn=collator)\n",
    "\n",
    "for batch in loader_with_collator:\n",
    "    images, targets = batch\n",
    "    print('batch 이미지 수: ', len(images))\n",
    "    print('1st 이미지 크기: ', images[0].shape)\n",
    "    print('2st 이미지 크기: ', images[1].shape)\n",
    "    print('1st target: ', targets[0])\n",
    "    print('2st target: ', targets[1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "eb979889",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "\n",
    "from torchvision import models\n",
    "from torchvision import ops\n",
    "from torchvision.models.detection import rpn\n",
    "from torchvision.models.detection import FasterRCNN\n",
    "\n",
    "# 사전 학습된 VGG16 모델의 Feature 부분만 backbone으로 사용\n",
    "backbone = models.vgg16(weights='VGG16_Weights.IMAGENET1K_V1').features\n",
    "\n",
    "# backbone의 출력 채널을 512로 설정\n",
    "backbone.out_channels = 512\n",
    "\n",
    "anchor_generator = rpn.AnchorGenerator(\n",
    "    sizes=((32, 64, 128, 256, 512),), # 앵커의 크기 지정\n",
    "    aspect_ratios=((0.5, 1.0, 2.0))   # 앵커의 가로세로 비율 지정\n",
    ")\n",
    "\n",
    "# ROI Pooling : 여러 스케일의 특징맵에서 ROI를 고정 크기로 변환\n",
    "roi_pooler = ops.MultiScaleRoIAlign(\n",
    "    featmap_names=['0'],    # 사용할 특징맵의 이름\n",
    "    output_size=(7, 7),     # 최종적으로 나갈 사이즈\n",
    "    sampling_ratio=2,       # 한 그리드 셀에서 몇개의 샘플을 활용해 데이터를 가져올거냐?, 샘플이 많을수록 원본에 가깝지만 연산량이 많아짐\n",
    ")\n",
    "\n",
    "device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "\n",
    "model = FasterRCNN(\n",
    "    backbone=backbone,\n",
    "    num_classes=3,   # 강아지, 고양이, 배경\n",
    "    rpn_anchor_generator=anchor_generator,\n",
    "    box_roi_pool=roi_pooler\n",
    ").to(device)  # 모델을 선택한 디바이스로 이동"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "766282eb",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "FasterRCNN(\n",
       "  (transform): GeneralizedRCNNTransform(\n",
       "      Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
       "      Resize(min_size=(800,), max_size=1333, mode='bilinear')\n",
       "  )\n",
       "  (backbone): Sequential(\n",
       "    (0): Conv2d(3, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (1): ReLU(inplace=True)\n",
       "    (2): Conv2d(64, 64, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (3): ReLU(inplace=True)\n",
       "    (4): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (5): Conv2d(64, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (6): ReLU(inplace=True)\n",
       "    (7): Conv2d(128, 128, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (8): ReLU(inplace=True)\n",
       "    (9): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (10): Conv2d(128, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (11): ReLU(inplace=True)\n",
       "    (12): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (13): ReLU(inplace=True)\n",
       "    (14): Conv2d(256, 256, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (15): ReLU(inplace=True)\n",
       "    (16): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (17): Conv2d(256, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (18): ReLU(inplace=True)\n",
       "    (19): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (20): ReLU(inplace=True)\n",
       "    (21): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (22): ReLU(inplace=True)\n",
       "    (23): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "    (24): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (25): ReLU(inplace=True)\n",
       "    (26): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (27): ReLU(inplace=True)\n",
       "    (28): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "    (29): ReLU(inplace=True)\n",
       "    (30): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
       "  )\n",
       "  (rpn): RegionProposalNetwork(\n",
       "    (anchor_generator): AnchorGenerator()\n",
       "    (head): RPNHead(\n",
       "      (conv): Sequential(\n",
       "        (0): Conv2dNormActivation(\n",
       "          (0): Conv2d(512, 512, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
       "          (1): ReLU(inplace=True)\n",
       "        )\n",
       "      )\n",
       "      (cls_logits): Conv2d(512, 15, kernel_size=(1, 1), stride=(1, 1))\n",
       "      (bbox_pred): Conv2d(512, 60, kernel_size=(1, 1), stride=(1, 1))\n",
       "    )\n",
       "  )\n",
       "  (roi_heads): RoIHeads(\n",
       "    (box_roi_pool): MultiScaleRoIAlign(featmap_names=['0'], output_size=(7, 7), sampling_ratio=2)\n",
       "    (box_head): TwoMLPHead(\n",
       "      (fc6): Linear(in_features=25088, out_features=1024, bias=True)\n",
       "      (fc7): Linear(in_features=1024, out_features=1024, bias=True)\n",
       "    )\n",
       "    (box_predictor): FastRCNNPredictor(\n",
       "      (cls_score): Linear(in_features=1024, out_features=3, bias=True)\n",
       "      (bbox_pred): Linear(in_features=1024, out_features=12, bias=True)\n",
       "    )\n",
       "  )\n",
       ")"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "7dc95fd8",
   "metadata": {},
   "outputs": [],
   "source": [
    "# 모델 학습을 위한 최적화 기법 설정 (SGD 사용)\n",
    "from torch import optim\n",
    "\n",
    "params = [p for p in model.parameters() if p.requires_grad]  # 학습 가능한 파라미터들만 선택\n",
    "optimizer = optim.SGD(params, lr=0.001, momentum=0.9, weight_decay=0.0005)\n",
    "# 학습률 스케줄러: 일정 에폭마다 학습률 감소\n",
    "lr_scheduler = optim.lr_scheduler.StepLR(optimizer, step_size=5, gamma=0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c8bc9171",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Epoch 1/5 - Training:   0%|          | 0/607 [00:00<?, ?it/s]\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'Image' object has no attribute 'to'",
     "output_type": "error",
     "traceback": [
      "\u001b[31m---------------------------------------------------------------------------\u001b[39m",
      "\u001b[31mAttributeError\u001b[39m                            Traceback (most recent call last)",
      "\u001b[36mCell\u001b[39m\u001b[36m \u001b[39m\u001b[32mIn[31]\u001b[39m\u001b[32m, line 16\u001b[39m\n\u001b[32m     10\u001b[39m train_bar = tqdm(train_dataloader, desc=\u001b[33mf\u001b[39m\u001b[33m\"\u001b[39m\u001b[33mEpoch \u001b[39m\u001b[38;5;132;01m{\u001b[39;00mepoch+\u001b[32m1\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m/\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mnum_epochs\u001b[38;5;132;01m}\u001b[39;00m\u001b[33m - Training\u001b[39m\u001b[33m\"\u001b[39m)\n\u001b[32m     11\u001b[39m \u001b[38;5;28;01mfor\u001b[39;00m images, targets \u001b[38;5;129;01min\u001b[39;00m train_bar:\n\u001b[32m     12\u001b[39m     \u001b[38;5;66;03m# 이미지와 타겟을 디바이스로 이동\u001b[39;00m\n\u001b[32m     13\u001b[39m     \u001b[38;5;66;03m# images.to(device)로는 처이가 안됨\u001b[39;00m\n\u001b[32m     14\u001b[39m \n\u001b[32m     15\u001b[39m     \u001b[38;5;66;03m# 데이터가 텐서 형태로 있지 않기 때문에\u001b[39;00m\n\u001b[32m---> \u001b[39m\u001b[32m16\u001b[39m     images = [\u001b[43mimg\u001b[49m\u001b[43m.\u001b[49m\u001b[43mto\u001b[49m(device) \u001b[38;5;28;01mfor\u001b[39;00m img \u001b[38;5;129;01min\u001b[39;00m images]\n\u001b[32m     17\u001b[39m     targets = [{k: v.to(device) \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m t.items()} \u001b[38;5;28;01mfor\u001b[39;00m t \u001b[38;5;129;01min\u001b[39;00m targets]\n\u001b[32m     19\u001b[39m     \u001b[38;5;66;03m# 모델에 입력하여 손실(loss) 계산\u001b[39;00m\n\u001b[32m     20\u001b[39m     \u001b[38;5;66;03m# model에서 output이 loss dict형태로 보냄\u001b[39;00m\n",
      "\u001b[31mAttributeError\u001b[39m: 'Image' object has no attribute 'to'"
     ]
    }
   ],
   "source": [
    "from tqdm import tqdm  # 진행 상황을 표시하기 위한 라이브러리\n",
    "\n",
    "# 학습 루프 (총 5 에폭 동안 학습)\n",
    "num_epochs = 5\n",
    "for epoch in range(num_epochs):\n",
    "    model.train()  # 모델을 학습 모드로 전환\n",
    "    total_loss = 0.0\n",
    "\n",
    "    # tqdm 진행바를 사용하여 학습 진행 상황 표시\n",
    "    train_bar = tqdm(train_dataloader, desc=f\"Epoch {epoch+1}/{num_epochs} - Training\")\n",
    "    for images, targets in train_bar:\n",
    "        # 이미지와 타겟을 디바이스로 이동\n",
    "        # images.to(device)로는 처리가 안됨\n",
    "        \n",
    "        # 데이터가 텐서 형태로 있지 않기 때문에\n",
    "        images = [img.to(device) for img in images]\n",
    "        targets = [{k: v.to(device) for k, v in t.items()} for t in targets]\n",
    "\n",
    "        # 모델에 입력하여 손실(loss) 계산\n",
    "        # model에서 output이 loss dict형태로 보냄\n",
    "        loss_dict = model(images, targets)\n",
    "        # loss_dict에서 값(val)만 추출해서 모두 sum\n",
    "        losses = sum(loss for loss in loss_dict.values())\n",
    "\n",
    "        optimizer.zero_grad()  # 기울기 초기화\n",
    "        losses.backward()      # 역전파 수행\n",
    "        optimizer.step()       # 파라미터 업데이트\n",
    "\n",
    "        total_loss += losses.item()\n",
    "        train_bar.set_postfix(loss=f\"{losses.item():.3f}\")\n",
    "\n",
    "    lr_scheduler.step()  # 에폭이 끝난 후 학습률 업데이트\n",
    "    avg_train_loss = total_loss / len(train_dataloader)\n",
    "    print(f\"Epoch: {epoch+1:2d}, Avg Train Loss: {avg_train_loss:.3f}\")\n",
    "\n",
    "    # 사용하지 않는 GPU 메모리 해제 (메모리 최적화)\n",
    "    torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a6bbf2b0",
   "metadata": {},
   "outputs": [],
   "source": [
    "### 테스트 및 시각화 관련 코드 ###\n",
    "\n",
    "import numpy as np\n",
    "from matplotlib import pyplot as plt\n",
    "from torchvision.transforms.functional import to_pil_image\n",
    "\n",
    "# 바운딩 박스를 그림으로 표시하는 함수\n",
    "def draw_bbox(ax, box, text, color):\n",
    "    \"\"\"\n",
    "    바운딩 박스를 이미지에 그려주는 함수.\n",
    "    :param ax: matplotlib Axes 객체\n",
    "    :param box: 바운딩 박스 좌표 (x_min, y_min, x_max, y_max)\n",
    "    :param text: 바운딩 박스 위에 표시할 텍스트\n",
    "    :param color: 바운딩 박스와 텍스트의 색상\n",
    "    \"\"\"\n",
    "    ax.add_patch(\n",
    "        plt.Rectangle(\n",
    "            xy=(box[0], box[1]),\n",
    "            width=box[2] - box[0],\n",
    "            height=box[3] - box[1],\n",
    "            fill=False,\n",
    "            edgecolor=color,\n",
    "            linewidth=2,\n",
    "        )\n",
    "    )\n",
    "    ax.annotate(\n",
    "        text=text,\n",
    "        xy=(box[0] - 5, box[1] - 5),\n",
    "        color=color,\n",
    "        weight=\"bold\",\n",
    "        fontsize=13,\n",
    "    )\n",
    "\n",
    "num_vis = 0\n",
    "threshold = 0.5  # 예측 점수 임계값: 이 값 이상인 예측만 시각화에 사용\n",
    "categories = test_dataset.categories  # COCO 데이터셋의 카테고리 정보 사용\n",
    "with torch.no_grad():  # 평가 시에는 기울기 계산을 하지 않습니다.\n",
    "    model.eval()       # 모델을 평가 모드로 전환\n",
    "    for images, targets in test_dataloader:\n",
    "        images = [image.to(device) for image in images]\n",
    "        outputs = model(images)  # 모델로부터 예측 결과 생성\n",
    "\n",
    "        # 예측 결과에서 바운딩 박스, 라벨, 점수를 CPU의 numpy 배열로 변환\n",
    "        boxes = outputs[0][\"boxes\"].to(\"cpu\").numpy()\n",
    "        labels = outputs[0][\"labels\"].to(\"cpu\").numpy()\n",
    "        scores = outputs[0][\"scores\"].to(\"cpu\").numpy()\n",
    "\n",
    "        # 임계값 이상의 예측만 선택\n",
    "        boxes = boxes[scores >= threshold].astype(np.int32)\n",
    "        labels = labels[scores >= threshold]\n",
    "        scores = scores[scores >= threshold]\n",
    "\n",
    "        # 시각화를 위한 플롯 설정 (8x8 크기의 그림)\n",
    "        fig = plt.figure(figsize=(8, 8))\n",
    "        ax = fig.add_subplot(1, 1, 1)\n",
    "        plt.imshow(to_pil_image(images[0]))  # 첫 번째 이미지를 PIL 이미지로 변환하여 출력\n",
    "\n",
    "        # 예측 결과(빨간색 박스) 시각화\n",
    "        for box, label, score in zip(boxes, labels, scores):\n",
    "            draw_bbox(ax, box, f\"{categories[label]} - {score:.4f}\", \"red\")\n",
    "\n",
    "        # 정답 어노테이션(파란색 박스) 시각화\n",
    "        tboxes = targets[0][\"boxes\"].numpy()\n",
    "        tlabels = targets[0][\"labels\"].numpy()\n",
    "        for box, label in zip(tboxes, tlabels):\n",
    "            draw_bbox(ax, box, f\"{categories[label]}\", \"blue\")\n",
    "\n",
    "        plt.show()  # 시각화 결과 출력\n",
    "        num_vis += 1\n",
    "        if num_vis == 5:\n",
    "            break\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "codeit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.13.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
